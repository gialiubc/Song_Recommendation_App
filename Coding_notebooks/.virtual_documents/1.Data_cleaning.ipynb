


# Import the required libraries and dependencies
import pandas as pd
import hvplot.pandas
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity


# Read `tracks.csv` file
tracks_df = pd.read_csv(Path("data/tracks.csv"))
# Check df
tracks_df.head()


# Keep the wanted columns
vec_tracks_df = tracks_df[[ 'id', 'track_uri','track_href','analysis_url','danceability', 'energy', 'loudness', 'speechiness',
       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
       'duration_ms','key_none', 'key_0', 'key_1', 'key_2', 'key_3',
       'key_4', 'key_5', 'key_6', 'key_7', 'key_8', 'key_9', 'key_10',
       'key_11', 'mode_minor', 'mode_major', 'time_signature_0',
       'time_signature_1', 'time_signature_2', 'time_signature_3',
       'time_signature_4', 'time_signature_5', 'time_signature_6',
       'time_signature_7']]


# Create scaler
scaler = StandardScaler()
# Fit dataset into scaler
scaled_vec_tracks = scaler.fit_transform(vec_tracks_df.drop(columns=['id', 'track_uri', 'track_href','analysis_url']))
# Transform scaled data to dataframe
scaled_vec_tracks_df = pd.DataFrame(scaled_vec_tracks)


# Write dataset to csv file
vec_tracks_df.to_csv('resources/vec_tracks.csv')


# Write scaled vectors to csv file
scaled_vec_tracks_df.to_csv('resources/scaled_vec_tracks.csv')


# Save the scaler for future use
import pickle
scalerfile = 'resources/scaler.sav'
pickle.dump(scaler, open(scalerfile, 'wb'))



